{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from autoencoder import NNAutoencoder\n",
    "from read_data import read_raw, read_and_perform, train_test_split, scalings\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leemos los archivos raw\n",
    "folder = os.path.join('..', 'Date')\n",
    "dataframes1 = read_raw(folder)\n",
    "folder = os.path.join('..', 'Date2')\n",
    "dataframes2 = read_raw(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Se procesaron 297 series de longitud 99\n",
      "[+] Se procesaron 32 series de longitud 99\n"
     ]
    }
   ],
   "source": [
    "# Convertimos todo en arrays de numpy con series del mismo largo\n",
    "data_1 = read_and_perform(dataframes1, row_range=300, col_range=(3,12), split= True)\n",
    "data_2 = read_and_perform(dataframes2, row_range=99, col_range=(2,5), split= False)\n",
    "# concatenamos todas las series\n",
    "data_total = np.vstack([data_1.T, data_2.T])\n",
    "\n",
    "# separamos train y test\n",
    "train, test = train_test_split(data_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] StandardScaler entrenado\n",
      "[+] Train shape (310, 99)\n",
      "[+] Test shape (19, 99)\n"
     ]
    }
   ],
   "source": [
    "# escalamos\n",
    "scaler = scalings(train)\n",
    "train = scaler.fit_transform(train)\n",
    "print(f'[+] Train shape {train.shape}')\n",
    "test = scaler.transform(test)\n",
    "print(f'[+] Test shape {test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar modelo para un solo espacio latente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss = 1.0036629438400269\n",
      "100 loss = 0.12055174261331558\n",
      "200 loss = 0.0933930054306984\n",
      "300 loss = 0.05316300690174103\n",
      "400 loss = 0.05084472522139549\n",
      "500 loss = 0.03961068019270897\n",
      "600 loss = 0.03843935579061508\n",
      "700 loss = 0.035866428166627884\n",
      "800 loss = 0.035514894872903824\n",
      "900 loss = 0.0345127247273922\n",
      "0 loss = 1.0055004358291626\n",
      "100 loss = 0.13980978727340698\n",
      "200 loss = 0.109782874584198\n",
      "300 loss = 0.1003403514623642\n",
      "400 loss = 0.07922305911779404\n",
      "500 loss = 0.05936254560947418\n",
      "600 loss = 0.048795104026794434\n",
      "700 loss = 0.047507770359516144\n",
      "800 loss = 0.036474116146564484\n",
      "900 loss = 0.04032644256949425\n",
      "0 loss = 1.0061428546905518\n",
      "100 loss = 0.16261130571365356\n",
      "200 loss = 0.1271541267633438\n",
      "300 loss = 0.12171897292137146\n",
      "400 loss = 0.10697949677705765\n",
      "500 loss = 0.08774914592504501\n",
      "600 loss = 0.0790974423289299\n",
      "700 loss = 0.0679035633802414\n",
      "800 loss = 0.058117374777793884\n",
      "900 loss = 0.05725330859422684\n",
      "0 loss = 1.003230094909668\n",
      "100 loss = 0.16563333570957184\n",
      "200 loss = 0.1336490511894226\n",
      "300 loss = 0.12073209136724472\n",
      "400 loss = 0.1324470192193985\n",
      "500 loss = 0.12328318506479263\n",
      "600 loss = 0.11913184076547623\n",
      "700 loss = 0.10031256824731827\n",
      "800 loss = 0.08970922976732254\n",
      "900 loss = 0.08166272938251495\n",
      "0 loss = 1.005089521408081\n",
      "100 loss = 0.19251340627670288\n",
      "200 loss = 0.1658279448747635\n",
      "300 loss = 0.14723297953605652\n",
      "400 loss = 0.13666334748268127\n",
      "500 loss = 0.13196350634098053\n",
      "600 loss = 0.12196969985961914\n",
      "700 loss = 0.11205267906188965\n",
      "800 loss = 0.10780730098485947\n",
      "900 loss = 0.10868949443101883\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "lr = 1e-3\n",
    "lat = 2\n",
    "drop = np.linspace(0.2,0.6,5)\n",
    "\n",
    "# Crear el directorio para guardar las imágenes si no existe\n",
    "path_img = os.path.join(\"..\",\"img\",\"Drop-Optimo\",f\"Loos-lat{lat}\")\n",
    "os.makedirs(path_img, exist_ok=True)\n",
    "\n",
    "for dr in drop:\n",
    "    # Entrenamiento\n",
    "    hist_train = []\n",
    "    hist_test = []\n",
    "    autoencoder = NNAutoencoder(99, lat, dr)\n",
    "    optimizer = torch.optim.Adam(autoencoder.parameters(), lr = lr)\n",
    "    criterio = torch.nn.MSELoss()\n",
    "\n",
    "    for e in range(epochs):\n",
    "        autoencoder.train()\n",
    "        x = torch.FloatTensor(train)\n",
    "        y_pred = autoencoder(x)\n",
    "        loss = criterio(y_pred, x)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if e%100 == 0:\n",
    "            print(e, \"loss =\",loss.item())\n",
    "        hist_train.append(loss.item())\n",
    "        with torch.no_grad():\n",
    "            autoencoder.eval()\n",
    "            x = torch.FloatTensor(test)\n",
    "            y_pred = autoencoder(x)\n",
    "            loss = criterio(y_pred, x)\n",
    "            hist_test.append(loss.item())\n",
    "\n",
    "    #guardo las img\n",
    "    plt.semilogy(hist_train, label = 'train loss')\n",
    "    plt.semilogy(hist_test, label = 'test loss')\n",
    "    plt.title(f\"Loss train -eval, drop = {dr}\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(path_img,f\"lat{lat}-drop{dr}.png\"), bbox_inches='tight')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopping\n",
    "\n",
    "Es  una técnica que permite detener el entrenamiento cuando el valor de pérdida en el conjunto de validación (test) comienza a aumentar. Esto ayuda a prevenir el sobreajuste y permite guardar el mejor modelo basado en el mínimo valor de pérdida en el conjunto de validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el directorio para guardar las imágenes si no existe\n",
    "path_model = os.path.join(\"..\",\"Save_Models\")\n",
    "os.makedirs(path_model, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train Loss: 1.0047, test Loss: 0.5473\n",
      "Epoch 100, train Loss: 0.1357, test Loss: 0.0970\n",
      "Epoch 200, train Loss: 0.1194, test Loss: 0.0933\n",
      "Epoch 300, train Loss: 0.1108, test Loss: 0.0859\n",
      "Epoch 400, train Loss: 0.1002, test Loss: 0.0640\n",
      "Epoch 500, train Loss: 0.0898, test Loss: 0.0457\n",
      "Epoch 600, train Loss: 0.0772, test Loss: 0.0510\n",
      "Epoch 700, train Loss: 0.0621, test Loss: 0.0448\n",
      "Epoch 800, train Loss: 0.0549, test Loss: 0.0333\n",
      "Epoch 900, train Loss: 0.0468, test Loss: 0.0316\n",
      "Best epoch was 487 with val loss 0.0414\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "lr = 1e-3\n",
    "dr = 0.3\n",
    "lat = 2\n",
    "\n",
    "# Entrenamiento\n",
    "hist_train = []\n",
    "hist_test = []\n",
    "best_test_loss = 100\n",
    "best_epoch = 0\n",
    "best_model = None\n",
    "espera = 100\n",
    "b = 0 #bandera\n",
    "\n",
    "autoencoder = NNAutoencoder(99, lat, dr)\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr = lr)\n",
    "criterio = torch.nn.MSELoss()\n",
    "\n",
    "for e in range(epochs):\n",
    "    autoencoder.train()\n",
    "    x = torch.FloatTensor(train)\n",
    "    y_pred = autoencoder(x)\n",
    "    loss = criterio(y_pred, x)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    hist_train.append(loss.item())\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        autoencoder.eval()\n",
    "        x = torch.FloatTensor(test)\n",
    "        y_pred = autoencoder(x)\n",
    "        test_loss = criterio(y_pred, x)\n",
    "        hist_test.append(test_loss.item())\n",
    "\n",
    "    if e%100 == 0:\n",
    "            print(f'Epoch {e}, train Loss: {loss.item():.4f}, test Loss: {test_loss.item():.4f}')\n",
    "\n",
    "    if test_loss < loss:\n",
    "        #guardo el mejor modelo\n",
    "        if test_loss < best_test_loss: #encuantra el primer minimo??\n",
    "            best_test_loss = test_loss\n",
    "            best_epoch = e\n",
    "            best_model = autoencoder.state_dict().copy() #copia del mejor modelo\n",
    "\n",
    "        if (e - best_epoch >= espera) and (b < 3):\n",
    "            m_epoch = best_epoch\n",
    "            m_test_loss = best_test_loss\n",
    "            torch.save(best_model, os.path.join(path_model,f\"model-lat{lat}.pth\"))\n",
    "            b += 1\n",
    "            plt.plot(m_epoch,m_test_loss,'x', color = \"red\")\n",
    "\n",
    "print(f'Best epoch was {m_epoch} with val loss {m_test_loss:.4f}')\n",
    "#guardo las img\n",
    "plt.semilogy(hist_train, label = 'train loss')\n",
    "plt.semilogy(hist_test, label = 'test loss')\n",
    "plt.plot(m_epoch,m_test_loss,'x', color = \"red\", label = \"best model\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(f\"dim_lat: {lat}, drop = {dr}\")\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(path_img,f\"BestModel-lat{lat}-drop{dr}.png\"), bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
