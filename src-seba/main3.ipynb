{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from autoencoder import NNAutoencoder\n",
    "from read_data import read_raw, read_and_perform, train_test_split, scalings\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leemos los archivos raw\n",
    "folder = os.path.join('..', 'Date')\n",
    "dataframes1 = read_raw(folder)\n",
    "folder = os.path.join('..', 'Date2')\n",
    "dataframes2 = read_raw(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertimos todo en arrays de numpy con series del mismo largo\n",
    "data_1 = read_and_perform(dataframes1, row_range=300, col_range=(3,12), split= True)\n",
    "data_2 = read_and_perform(dataframes2, row_range=99, col_range=(2,5), split= False)\n",
    "# concatenamos todas las series\n",
    "data_total = np.vstack([data_1.T, data_2.T])\n",
    "\n",
    "# separamos train y test\n",
    "train, test = train_test_split(data_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# escalamos\n",
    "scaler = scalings(train)\n",
    "train = scaler.fit_transform(train)\n",
    "print(f'[+] Train shape {train.shape}')\n",
    "test = scaler.transform(test)\n",
    "print(f'[+] Test shape {test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar modelo para un solo espacio latente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "lr = 1e-3\n",
    "lat = 5\n",
    "drop = np.linspace(0.2,0.6,5)\n",
    "\n",
    "# Crear el directorio para guardar las imágenes si no existe\n",
    "path_img = os.path.join(\"..\",\"img\",\"Drop-Optimo\",f\"Loos-lat{lat}\")\n",
    "os.makedirs(path_img, exist_ok=True)\n",
    "\n",
    "for dr in drop:\n",
    "    # Entrenamiento\n",
    "    hist_train = []\n",
    "    hist_test = []\n",
    "    autoencoder = NNAutoencoder(99, lat, dr)\n",
    "    optimizer = torch.optim.Adam(autoencoder.parameters(), lr = lr)\n",
    "    criterio = torch.nn.MSELoss()\n",
    "\n",
    "    for e in range(epochs):\n",
    "        autoencoder.train()\n",
    "        x = torch.FloatTensor(train)\n",
    "        y_pred = autoencoder(x)\n",
    "        loss = criterio(y_pred, x)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if e%100 == 0:\n",
    "            print(e, \"loss =\",loss.item())\n",
    "        hist_train.append(loss.item())\n",
    "        with torch.no_grad():\n",
    "            autoencoder.eval()\n",
    "            x = torch.FloatTensor(test)\n",
    "            y_pred = autoencoder(x)\n",
    "            loss = criterio(y_pred, x)\n",
    "            hist_test.append(loss.item())\n",
    "\n",
    "    #guardo las img\n",
    "    plt.semilogy(hist_train, label = 'train loss')\n",
    "    plt.semilogy(hist_test, label = 'test loss')\n",
    "    plt.title(f\"Loss train -eval, drop = {dr}\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(path_img,f\"lat{lat}-drop{dr}.png\"), bbox_inches='tight')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopping\n",
    "\n",
    "Es  una técnica que permite detener el entrenamiento cuando el valor de pérdida en el conjunto de validación (test) comienza a aumentar. Esto ayuda a prevenir el sobreajuste y permite guardar el mejor modelo basado en el mínimo valor de pérdida en el conjunto de validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el directorio para guardar las imágenes si no existe\n",
    "path_model = os.path.join(\"..\",\"Save_Models\")\n",
    "os.makedirs(path_model, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "lr = 1e-3\n",
    "dr = 0.2\n",
    "lat = 3\n",
    "\n",
    "# Crear el directorio para guardar las imágenes si no existe\n",
    "path_img = os.path.join(\"..\",\"img\",\"Drop-Optimo\",f\"Loos-lat{lat}\")\n",
    "os.makedirs(path_img, exist_ok=True)\n",
    "\n",
    "# Entrenamiento\n",
    "hist_train = []\n",
    "hist_test = []\n",
    "best_test_loss = 100\n",
    "best_epoch = 0\n",
    "best_model = None\n",
    "espera = 100\n",
    "b = 0 #bandera\n",
    "\n",
    "autoencoder = NNAutoencoder(99, lat, dr)\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr = lr)\n",
    "criterio = torch.nn.MSELoss()\n",
    "\n",
    "for e in range(epochs):\n",
    "    autoencoder.train()\n",
    "    x = torch.FloatTensor(train)\n",
    "    y_pred = autoencoder(x)\n",
    "    loss = criterio(y_pred, x)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    hist_train.append(loss.item())\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        autoencoder.eval()\n",
    "        x = torch.FloatTensor(test)\n",
    "        y_pred = autoencoder(x)\n",
    "        test_loss = criterio(y_pred, x)\n",
    "        hist_test.append(test_loss.item())\n",
    "\n",
    "    if e%100 == 0:\n",
    "            print(f'Epoch {e}, train Loss: {loss.item():.4f}, test Loss: {test_loss.item():.4f}')\n",
    "\n",
    "    if test_loss < loss:\n",
    "        #guardo el mejor modelo\n",
    "        if test_loss < best_test_loss: #encuantra el primer minimo??\n",
    "            best_test_loss = test_loss\n",
    "            best_epoch = e\n",
    "            best_model = autoencoder.state_dict().copy() #copia del mejor modelo\n",
    "\n",
    "        if (e - best_epoch >= espera) and (b < 3):\n",
    "            m_epoch = best_epoch\n",
    "            m_test_loss = best_test_loss\n",
    "            torch.save(best_model, os.path.join(path_model,f\"model-lat{lat}.pth\"))\n",
    "            b += 1\n",
    "            plt.plot(m_epoch,m_test_loss,'x', color = \"red\")\n",
    "\n",
    "print(f'Best epoch was {m_epoch} with val loss {m_test_loss:.4f}')\n",
    "#guardo las img\n",
    "plt.semilogy(hist_train, label = 'train loss')\n",
    "plt.semilogy(hist_test, label = 'test loss')\n",
    "plt.plot(m_epoch,m_test_loss,'x', color = \"red\", label = \"best model\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(f\"dim_lat: {lat}, drop = {dr}\")\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(path_img,f\"BestModel-lat{lat}-drop{dr}.png\"), bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verificamos que el path sea correcto y en caso que asi sea, vemos los nombres de archivos dentro\n",
    "if os.path.isdir(os.path.join(\"..\",\"Save_Models\")):\n",
    "    filename = os.listdir(os.path.join(\"..\",\"Save_Models\")) # guardamos la lista\n",
    "    print(filename)\n",
    "\n",
    "dicc_model ={}\n",
    "for f in filename:\n",
    "    indl=9\n",
    "    indr=f.find('.pth')\n",
    "    #print(int(f[indl:indr]))\n",
    "    dicc_model[int(f[indl:indr])] = f\n",
    "\n",
    "# Crear el directorio para guardar las imágenes MAES si no existe\n",
    "path_maes = os.path.join(\"..\",\"img\",\"MAES\")\n",
    "os.makedirs(path_maes, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_list = [] #media de cada esp. lat\n",
    "std_list = [] #desviacion de cada esp. lat\n",
    "dicc_drop = {}\n",
    "drop = [0.3,0.3,0.3,0.4,0.2,0.4,0.3,0.2,0.2,0.3,0.3,0.4,0.3,0.2,0.4,0.4,0.3,0.2,0.4,0.2]\n",
    "for i, d in enumerate(drop):\n",
    "    dicc_drop[i+1] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAES_list = []\n",
    "lat = 20\n",
    "dr = dicc_drop[lat]\n",
    "\n",
    "autoencoder = NNAutoencoder(99, lat, dr)\n",
    "autoencoder.load_state_dict(torch.load(os.path.join(\"..\",\"Save_Models\",dicc_model[lat])))\n",
    "\n",
    "for i in range(500):\n",
    "    autoencoder.train()\n",
    "    x = torch.FloatTensor(test)\n",
    "    y_pred = autoencoder(x)\n",
    "    sample = scaler.inverse_transform(x)\n",
    "    sample_pred = scaler.inverse_transform(y_pred.detach().numpy())\n",
    "    #print(sample.shape, sample_pred.shape)\n",
    "    average_MAES = np.abs(sample-sample_pred).mean()\n",
    "    MAES_list.append(average_MAES)\n",
    "    print(f'DIM= {i} & Average MAE= {average_MAES:.3f}')\n",
    "\n",
    "array = np.array(MAES_list)\n",
    "mean_list.append(np.mean(array))\n",
    "std_list.append(np.std(array))\n",
    "\n",
    "# Crear el histograma\n",
    "#plt.figure(figsize=(10, 6))\n",
    "sns.histplot(MAES_list, kde = True, bins=50, edgecolor='black')\n",
    "plt.xlabel('MAES')\n",
    "plt.ylabel('f')\n",
    "plt.title(f'MAES, lat{lat} drop {dr}')\n",
    "plt.savefig(os.path.join(path_maes,f\"MAES{lat}-drop{dr}.png\"), bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_list[21], std_list[21])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAE vs Dim Latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = np.linspace(1,20,20)\n",
    "fig, ax = plt.subplots(figsize = (10,6))\n",
    "ax.errorbar(latent,mean_list,std_list,fmt='o', linewidth=2, capsize=6)\n",
    "ax.plot(latent,mean_list)\n",
    "ax.set_ylabel(\"MAES\")\n",
    "ax.set_xlabel(\"Laten Dim\")\n",
    "ax.set_xticks(np.arange(0, 22, 1))\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autoencoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
